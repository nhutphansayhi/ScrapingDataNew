================================================================================
Google Colab Notebook Template for arXiv Scraper
================================================================================

Copy and paste this into a new Google Colab notebook:


# ============================================================================
# CELL 1: Setup and Installation
# ============================================================================

# Install required packages
!pip install arxiv==2.1.0 requests==2.31.0 pandas==2.0.3 -q

print("✓ Dependencies installed")


# ============================================================================
# CELL 2: Upload Source Code
# ============================================================================

# Option A: Upload from local machine
from google.colab import files
print("Please upload the 23127240.zip file containing your source code")
uploaded = files.upload()

# Extract the uploaded zip
!unzip -q 23127240.zip
!ls -la 23127240/

print("✓ Source code uploaded and extracted")


# Option B: Clone from GitHub (if you've uploaded to GitHub)
# !git clone https://github.com/yourusername/arxiv-scraper.git
# !mv arxiv-scraper 23127240


# ============================================================================
# CELL 3: Verify Installation
# ============================================================================

# Check directory structure
!tree 23127240/ -L 2 || find 23127240/ -maxdepth 2

# Verify files exist
import os
required_files = [
    '23127240/src/main.py',
    '23127240/src/arxiv_scraper.py',
    '23127240/src/reference_scraper.py',
    '23127240/src/requirements.txt'
]

for f in required_files:
    if os.path.exists(f):
        print(f"✓ {f}")
    else:
        print(f"✗ {f} MISSING!")


# ============================================================================
# CELL 4: Test Run (Small Range)
# ============================================================================

# Test with just 3 papers to verify everything works
%cd /content/23127240/src

!python main.py --start-ym 2208 --start-id 11941 --end-ym 2208 --end-id 11943


# ============================================================================
# CELL 5: Check Test Results
# ============================================================================

# View test statistics
import json

with open('../23127240_data/scraping_stats.json', 'r') as f:
    stats = json.load(f)
    print(json.dumps(stats, indent=2))

# List scraped papers
!ls -lh ../23127240_data/


# ============================================================================
# CELL 6: Run Full Scraper (WARNING: Takes 2-4 hours!)
# ============================================================================

# ⚠️ This will run the full scraping pipeline
# Make sure Colab doesn't disconnect during execution
# Consider using Colab Pro for longer runtime

%cd /content/23127240/src

# Run with default settings (full assignment range: 2208.11941 to 2209.11937)
!python main.py


# ============================================================================
# CELL 7: Monitor Progress (Run in parallel with Cell 6)
# ============================================================================

# View last 20 lines of log
!tail -n 20 logs/scraper.log

# Or continuously monitor (will keep updating)
# Press Stop button to stop monitoring
!tail -f logs/scraper.log


# ============================================================================
# CELL 8: View Final Statistics
# ============================================================================

import json
import pandas as pd

# Load statistics
with open('../23127240_data/scraping_stats.json', 'r') as f:
    stats = json.load(f)

print("="*80)
print("FINAL SCRAPING STATISTICS")
print("="*80)
print(json.dumps(stats, indent=2))

# Calculate averages
pipeline_stats = stats['pipeline']
print("\n" + "="*80)
print("SUMMARY")
print("="*80)
print(f"Total papers: {pipeline_stats['total_papers']}")
print(f"Successful: {pipeline_stats['successful_papers']}")
print(f"Failed: {pipeline_stats['failed_papers']}")
print(f"Success rate: {pipeline_stats['successful_papers']/max(1, pipeline_stats['total_papers'])*100:.1f}%")
print(f"Total runtime: {pipeline_stats['total_runtime']:.2f}s ({pipeline_stats['total_runtime']/60:.2f} min)")

if pipeline_stats['paper_runtimes']:
    avg_runtime = sum(pipeline_stats['paper_runtimes']) / len(pipeline_stats['paper_runtimes'])
    print(f"Average time per paper: {avg_runtime:.2f}s")

if pipeline_stats['reference_counts']:
    avg_refs = sum(pipeline_stats['reference_counts']) / len(pipeline_stats['reference_counts'])
    print(f"Average references per paper: {avg_refs:.2f}")

if pipeline_stats['paper_sizes_after']:
    avg_size = sum(pipeline_stats['paper_sizes_after']) / len(pipeline_stats['paper_sizes_after'])
    total_size = sum(pipeline_stats['paper_sizes_after'])
    print(f"Average paper size: {avg_size/1024:.2f} KB")
    print(f"Total data size: {total_size/1024/1024:.2f} MB")


# ============================================================================
# CELL 9: Inspect Sample Paper
# ============================================================================

# Look at a sample paper's structure
!echo "=== Directory Structure ==="
!tree ../23127240_data/2208-11941/ || find ../23127240_data/2208-11941/

# View metadata
!echo "\n=== Metadata ==="
!cat ../23127240_data/2208-11941/metadata.json

# View references
!echo "\n=== References ==="
!cat ../23127240_data/2208-11941/references.json

# View BibTeX
!echo "\n=== BibTeX ==="
!cat ../23127240_data/2208-11941/references.bib


# ============================================================================
# CELL 10: Memory Usage Analysis
# ============================================================================

# Check memory usage
!free -h

# Check disk usage
!df -h

# Check data directory size
!du -sh ../23127240_data/
!du -h ../23127240_data/ | sort -hr | head -20


# ============================================================================
# CELL 11: Package Results
# ============================================================================

# Create zip file for submission
%cd /content

# Zip the data folder
!zip -r 23127240_data.zip 23127240_data/

# Check zip size
!ls -lh 23127240_data.zip

print("\n✓ Data packaged successfully")


# ============================================================================
# CELL 12: Download Results
# ============================================================================

# Download the zip file
from google.colab import files

files.download('23127240_data.zip')

print("✓ Download started - check your browser downloads")


# ============================================================================
# CELL 13: Create Source Code Submission Package
# ============================================================================

%cd /content

# Copy only source files (no data)
!mkdir -p 23127240_submission/src
!cp 23127240/src/*.py 23127240_submission/src/
!cp 23127240/src/requirements.txt 23127240_submission/src/
!cp 23127240/README.md 23127240_submission/
!cp 23127240/Report.doc 23127240_submission/

# Zip source code for Moodle submission
!zip -r 23127240_source.zip 23127240_submission/

!ls -lh 23127240_source.zip

# Download source code package
files.download('23127240_source.zip')

print("✓ Source code package ready for Moodle submission")


# ============================================================================
# CELL 14: Performance Benchmarking
# ============================================================================

# Time a single paper scrape for benchmarking
import time

%cd /content/23127240/src

start = time.time()
!python main.py --start-ym 2208 --start-id 11950 --end-ym 2208 --end-id 11950
end = time.time()

print(f"\nBenchmark: Single paper took {end-start:.2f} seconds")


================================================================================
END OF NOTEBOOK TEMPLATE
================================================================================

NOTES:
- Run cells in order
- Cell 6 (full scraper) will take 2-4 hours - make sure Colab stays connected
- You can run Cell 7 while Cell 6 is running to monitor progress
- Save your work frequently
- Consider using Colab Pro for longer runtime limits

TIPS FOR VIDEO RECORDING:
1. Run Cells 1-5 to show setup and testing
2. Show Cell 7 (logs) to demonstrate the scraping process
3. Show Cell 8 (statistics) for results
4. Show Cell 9 (sample paper) to show data structure
5. Total video should be under 120 seconds

