# HÆ¯á»šNG DáºªN CHáº Y SCRAPER TRÃŠN GOOGLE COLAB Tá»ª Äáº¦U

## ğŸ“‹ YÃŠU Cáº¦U LAB 1

Theo Ä‘á» bÃ i, báº¡n cáº§n:
- **Testbed**: Google Colab CPU-only mode
- **Äo lÆ°á»ng**: Wall time (end-to-end), Memory footprint (max RAM, disk usage)
- **Thu tháº­p**: TeX sources, metadata.json, references.json
- **Tá»‘i Æ°u**: XÃ³a figures Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c

---

## ğŸš€ BÆ¯á»šC 1: Má» GOOGLE COLAB

1. Truy cáº­p: https://colab.research.google.com/
2. ÄÄƒng nháº­p Google Account
3. **File > New notebook**
4. Äá»•i tÃªn: `ArXiv_Scraper_23127240.ipynb`

---

## âš™ï¸ BÆ¯á»šC 2: SETUP RUNTIME (QUAN TRá»ŒNG!)

### Chuyá»ƒn sang CPU-only mode:
1. **Runtime > Change runtime type**
2. **Hardware accelerator > None** (CPU-only theo yÃªu cáº§u Lab)
3. Click **Save**

### Cell 1: Kiá»ƒm tra Runtime

```python
import psutil
import platform

print("=" * 60)
print("THÃ”NG TIN RUNTIME - Lab 1 Requirements")
print("=" * 60)
print(f"OS: {platform.system()} {platform.release()}")
print(f"CPU cores: {psutil.cpu_count()}")
print(f"Total RAM: {psutil.virtual_memory().total / (1024**3):.2f} GB")
print(f"Total Disk: {psutil.disk_usage('/').total / (1024**3):.2f} GB")
print("=" * 60)

# Kiá»ƒm tra CPU-only (khÃ´ng cÃ³ GPU)
try:
    import torch
    if torch.cuda.is_available():
        print("\nâš ï¸  WARNING: GPU detected!")
        print("ğŸ‘‰ Lab yÃªu cáº§u CPU-only mode")
        print("   Runtime > Change runtime type > Hardware accelerator > None")
    else:
        print("\nâœ… CPU-only mode - ÄÃºng yÃªu cáº§u Lab 1")
except:
    print("\nâœ… CPU-only mode - ÄÃºng yÃªu cáº§u Lab 1")
```

**Cháº¡y cell nÃ y vÃ  Ä‘áº£m báº£o tháº¥y "âœ… CPU-only mode"**

---

## ğŸ“¥ BÆ¯á»šC 3: CLONE REPOSITORY

### Cell 2: Clone source code

```python
# Clone repository tá»« GitHub
!git clone https://github.com/nhutphansayhi/ScrapingData.git

# Di chuyá»ƒn vÃ o thÆ° má»¥c project
%cd ScrapingData/23127240

# Kiá»ƒm tra cáº¥u trÃºc
print("\nğŸ“ Project Structure:")
!ls -la

print("\nğŸ“‚ Source Code:")
!ls -la src/
```

---

## ğŸ”§ BÆ¯á»šC 4: CÃ€I Äáº¶T DEPENDENCIES

### Cell 3: Install requirements

```python
# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
!pip install -q -r src/requirements.txt

# Verify installation
import arxiv
import requests
from bs4 import BeautifulSoup
import bibtexparser
import psutil
import json
import time
from datetime import datetime

print("âœ… Táº¥t cáº£ thÆ° viá»‡n Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t thÃ nh cÃ´ng!")
print("\nCÃ¡c thÆ° viá»‡n chÃ­nh:")
print("  - arxiv: Entry discovery")
print("  - requests: HTTP requests")
print("  - beautifulsoup4: HTML parsing")
print("  - bibtexparser: BibTeX parsing")
print("  - psutil: Performance monitoring")
```

---

## ğŸ“Š BÆ¯á»šC 5: SETUP PERFORMANCE MONITOR

### Cell 4: Táº¡o Performance Monitor (theo yÃªu cáº§u Lab 1)

```python
import psutil
import time
import os
from datetime import datetime

class PerformanceMonitor:
    """
    Monitor Ä‘á»ƒ Ä‘o wall time vÃ  memory footprint theo yÃªu cáº§u Lab 1
    """
    
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.max_ram_mb = 0
        self.initial_disk_mb = 0
        self.paper_count = 0
        self.paper_times = []
        
    def start(self):
        """Báº¯t Ä‘áº§u Ä‘o wall time (end-to-end)"""
        self.start_time = time.time()
        self.initial_disk_mb = psutil.disk_usage('/').used / (1024**2)
        
        print("=" * 70)
        print("ğŸš€ Báº®T Äáº¦U SCRAPING - LAB 1 PERFORMANCE MONITORING")
        print("=" * 70)
        print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Initial RAM: {psutil.virtual_memory().used / (1024**2):.2f} MB")
        print(f"Initial Disk: {self.initial_disk_mb:.2f} MB")
        print("=" * 70)
        
    def update_metrics(self):
        """Cáº­p nháº­t metrics trong quÃ¡ trÃ¬nh scraping"""
        # Äo RAM usage
        current_ram_mb = psutil.virtual_memory().used / (1024**2)
        self.max_ram_mb = max(self.max_ram_mb, current_ram_mb)
        
    def finish(self, output_dir="23127240_data"):
        """Káº¿t thÃºc vÃ  hiá»ƒn thá»‹ metrics"""
        self.end_time = time.time()
        total_time_sec = self.end_time - self.start_time
        
        # TÃ­nh disk increase
        final_disk_mb = psutil.disk_usage('/').used / (1024**2)
        disk_increase_mb = final_disk_mb - self.initial_disk_mb
        
        # TÃ­nh output size
        output_size_mb = 0
        if os.path.exists(output_dir):
            total_bytes = sum(
                os.path.getsize(os.path.join(dp, f))
                for dp, dn, filenames in os.walk(output_dir)
                for f in filenames
            )
            output_size_mb = total_bytes / (1024**2)
        
        # Hiá»ƒn thá»‹ report
        print("\n" + "=" * 70)
        print("ğŸ“Š PERFORMANCE METRICS - LAB 1 REQUIREMENTS")
        print("=" * 70)
        
        print("\nğŸ¯ TESTBED: Google Colab CPU-only mode")
        print("-" * 70)
        
        # 1. RUNNING TIME (Wall Time - End-to-End)
        print("\nâ±ï¸  RUNNING TIME:")
        print(f"   â€¢ Total wall time: {total_time_sec:.2f} seconds")
        print(f"   â€¢ In minutes: {total_time_sec/60:.2f} minutes")
        print(f"   â€¢ In hours: {total_time_sec/3600:.2f} hours")
        
        if self.paper_times:
            avg_time = sum(self.paper_times) / len(self.paper_times)
            print(f"   â€¢ Papers processed: {len(self.paper_times)}")
            print(f"   â€¢ Average per paper: {avg_time:.2f} seconds")
        
        # 2. MEMORY FOOTPRINT
        print("\nğŸ’¾ MEMORY FOOTPRINT:")
        print(f"   â€¢ Maximum RAM used: {self.max_ram_mb:.2f} MB ({self.max_ram_mb/1024:.2f} GB)")
        print(f"   â€¢ Current RAM: {psutil.virtual_memory().used/(1024**2):.2f} MB")
        print(f"   â€¢ Disk increase: {disk_increase_mb:.2f} MB ({disk_increase_mb/1024:.2f} GB)")
        print(f"   â€¢ Output data size: {output_size_mb:.2f} MB ({output_size_mb/1024:.2f} GB)")
        
        print("\n" + "=" * 70)
        print("âœ… Copy metrics nÃ y vÃ o Report.docx!")
        print("=" * 70)
        
        # LÆ°u metrics
        metrics = {
            'testbed': 'Google Colab CPU-only',
            'total_wall_time_seconds': total_time_sec,
            'total_wall_time_minutes': total_time_sec / 60,
            'total_wall_time_hours': total_time_sec / 3600,
            'max_ram_mb': self.max_ram_mb,
            'max_ram_gb': self.max_ram_mb / 1024,
            'disk_increase_mb': disk_increase_mb,
            'disk_increase_gb': disk_increase_mb / 1024,
            'output_size_mb': output_size_mb,
            'output_size_gb': output_size_mb / 1024,
            'papers_processed': len(self.paper_times),
            'avg_time_per_paper': sum(self.paper_times)/len(self.paper_times) if self.paper_times else 0,
            'start_time': datetime.fromtimestamp(self.start_time).isoformat(),
            'end_time': datetime.fromtimestamp(self.end_time).isoformat()
        }
        
        return metrics

# Khá»Ÿi táº¡o monitor
monitor = PerformanceMonitor()
print("âœ… Performance Monitor Ä‘Ã£ sáºµn sÃ ng theo yÃªu cáº§u Lab 1!")
```

---

## ğŸ”„ BÆ¯á»šC 6: CHáº Y SCRAPER (END-TO-END)

### Cell 5A: Kiá»ƒm tra files trong src/

```python
# Di chuyá»ƒn vÃ o thÆ° má»¥c src
%cd /content/ScrapingData/23127240/src

# Kiá»ƒm tra files Python
print("ğŸ“‚ Python files trong src/:")
!ls -la *.py

# Kiá»ƒm tra ná»™i dung thÆ° má»¥c
print("\nï¿½ All files:")
!ls -la
```

### Cell 5B: Cháº¡y scraper trá»±c tiáº¿p (RECOMMENDED)

**CÃ¡ch nÃ y KHÃ”NG Cáº¦N import module, cháº¡y trá»±c tiáº¿p qua terminal**

```python
import subprocess
import sys
import os

# Di chuyá»ƒn vÃ o thÆ° má»¥c project root
%cd /content/ScrapingData/23127240

# Báº®T Äáº¦U ÄO WALL TIME (End-to-End)
monitor.start()

try:
    print("\nğŸ”„ Äang cháº¡y scraper...")
    print("\nQuy trÃ¬nh (theo Ä‘á» bÃ i Lab 1):")
    print("  1ï¸âƒ£  Entry Discovery: arXiv API")
    print("  2ï¸âƒ£  Source Download: .tar.gz extraction")
    print("  3ï¸âƒ£  Figure Removal: XÃ³a png, jpg, pdf, eps")
    print("  4ï¸âƒ£  Reference Crawling: Semantic Scholar API")
    print("  5ï¸âƒ£  Data Organization: tex/, metadata.json, references.json")
    print("\n" + "-" * 70)
    
    # Cháº¡y scraper báº±ng subprocess (cháº¡y nhÆ° terminal command)
    # TÃ¬m file Python chÃ­nh trong src/
    src_dir = "src"
    python_files = [f for f in os.listdir(src_dir) if f.endswith('.py')]
    
    print(f"\nğŸ“„ Python files found: {python_files}")
    
    # Thá»­ cÃ¡c tÃªn file phá»• biáº¿n
    main_file = None
    for filename in ['main.py', 'scraper.py', 'arxiv_scraper.py', 'run.py']:
        if filename in python_files:
            main_file = filename
            break
    
    if main_file:
        print(f"\nğŸš€ Cháº¡y: {main_file}")
        
        # Cháº¡y script
        result = subprocess.run(
            [sys.executable, os.path.join(src_dir, main_file)],
            capture_output=True,
            text=True,
            cwd='/content/ScrapingData/23127240'
        )
        
        # Hiá»ƒn thá»‹ output
        print(result.stdout)
        if result.stderr:
            print("STDERR:", result.stderr)
        
        if result.returncode == 0:
            print("\nâœ… Scraping hoÃ n táº¥t!")
        else:
            print(f"\nâš ï¸  Script exited with code: {result.returncode}")
    else:
        print("\nâŒ KhÃ´ng tÃ¬m tháº¥y file main script!")
        print("ğŸ“ Danh sÃ¡ch files:")
        for f in python_files:
            print(f"  - {f}")
        raise FileNotFoundError("Main script not found")
    
    # Cáº­p nháº­t metrics
    monitor.update_metrics()
    
except KeyboardInterrupt:
    print("\nâš ï¸  Scraping bá»‹ ngáº¯t bá»Ÿi user")
except Exception as e:
    print(f"\nâŒ Lá»—i: {e}")
    import traceback
    traceback.print_exc()
finally:
    # Káº¾T THÃšC ÄO WALL TIME
    metrics = monitor.finish()
    
    # LÆ°u metrics
    with open('performance_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print("\nğŸ’¾ Metrics Ä‘Ã£ lÆ°u vÃ o: performance_metrics.json")
```

### Cell 5C: HOáº¶C cháº¡y trá»±c tiáº¿p báº±ng ! (Ä‘Æ¡n giáº£n nháº¥t)

```python
# Báº®T Äáº¦U ÄO WALL TIME
monitor.start()

# Di chuyá»ƒn vÃ o src vÃ  cháº¡y
%cd /content/ScrapingData/23127240/src

# Cháº¡y trá»±c tiáº¿p (thay main.py báº±ng tÃªn file thá»±c táº¿ náº¿u khÃ¡c)
!python3 main.py

# Vá» thÆ° má»¥c gá»‘c
%cd /content/ScrapingData/23127240

# Káº¾T THÃšC ÄO WALL TIME
metrics = monitor.finish()

# LÆ°u metrics
with open('performance_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=2)

print("\nğŸ’¾ Metrics Ä‘Ã£ lÆ°u vÃ o: performance_metrics.json")
```

**LÆ°u Ã½**: 
- Cháº¡y Cell 5A trÆ°á»›c Ä‘á»ƒ xem tÃªn file chÃ­nh xÃ¡c
- Náº¿u file khÃ´ng pháº£i `main.py`, thay Ä‘á»•i tÃªn trong Cell 5C
- Script sáº½ cháº¡y tá»« vÃ i phÃºt Ä‘áº¿n vÃ i giá» tÃ¹y sá»‘ lÆ°á»£ng papers
- Monitor tá»± Ä‘á»™ng Ä‘o wall time vÃ  memory footprint
- Náº¿u gáº·p lá»—i 429 (rate limit), script sáº½ tá»± Ä‘á»™ng retry

---

## ğŸ“ BÆ¯á»šC 7: KIá»‚M TRA OUTPUT

### Cell 6: Verify data structure

```python
import os
import json

def verify_data_structure(data_dir="23127240_data"):
    """Kiá»ƒm tra cáº¥u trÃºc dá»¯ liá»‡u theo yÃªu cáº§u Lab 1"""
    
    print("=" * 70)
    print("ğŸ“ KIá»‚M TRA Cáº¤U TRÃšC Dá»® LIá»†U (theo Ä‘á» bÃ i)")
    print("=" * 70)
    
    if not os.path.exists(data_dir):
        print(f"âŒ ThÆ° má»¥c {data_dir} khÃ´ng tá»“n táº¡i!")
        return None
    
    # List papers
    papers = [d for d in os.listdir(data_dir) 
              if os.path.isdir(os.path.join(data_dir, d)) and d.startswith('23')]
    papers = sorted(papers)
    
    print(f"\nğŸ“Š Tá»•ng sá»‘ papers: {len(papers)}")
    
    stats = {
        'total_papers': len(papers),
        'papers_with_tex': 0,
        'papers_with_metadata': 0,
        'papers_with_references': 0,
        'total_versions': 0,
        'total_tex_files': 0,
        'total_bib_files': 0
    }
    
    # Check first 5 papers in detail
    print("\nğŸ“„ Chi tiáº¿t 5 papers Ä‘áº§u tiÃªn:")
    for paper_id in papers[:5]:
        paper_path = os.path.join(data_dir, paper_id)
        print(f"\n  {paper_id}:")
        
        # Check tex/ folder
        tex_path = os.path.join(paper_path, "tex")
        if os.path.exists(tex_path):
            versions = [v for v in os.listdir(tex_path) 
                       if os.path.isdir(os.path.join(tex_path, v))]
            if versions:
                stats['papers_with_tex'] += 1
                stats['total_versions'] += len(versions)
                print(f"    âœ… tex/ - {len(versions)} version(s)")
                
                # Count .tex and .bib files
                for version in versions:
                    version_path = os.path.join(tex_path, version)
                    for root, dirs, files in os.walk(version_path):
                        tex_files = [f for f in files if f.endswith('.tex')]
                        bib_files = [f for f in files if f.endswith('.bib')]
                        stats['total_tex_files'] += len(tex_files)
                        stats['total_bib_files'] += len(bib_files)
            else:
                print(f"    âš ï¸  tex/ empty (no extractable TeX)")
        else:
            print(f"    âŒ tex/ missing")
        
        # Check metadata.json
        metadata_path = os.path.join(paper_path, "metadata.json")
        if os.path.exists(metadata_path):
            stats['papers_with_metadata'] += 1
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
                title = metadata.get('title', 'N/A')
                print(f"    âœ… metadata.json")
                print(f"       Title: {title[:50]}...")
        else:
            print(f"    âŒ metadata.json missing")
        
        # Check references.json
        ref_path = os.path.join(paper_path, "references.json")
        if os.path.exists(ref_path):
            stats['papers_with_references'] += 1
            with open(ref_path, 'r', encoding='utf-8') as f:
                refs = json.load(f)
                print(f"    âœ… references.json - {len(refs)} refs with arXiv IDs")
        else:
            print(f"    âš ï¸  references.json missing (paper cÃ³ thá»ƒ khÃ´ng cÃ³ refs)")
    
    # Summary
    print("\n" + "=" * 70)
    print("ğŸ“Š STATISTICS SUMMARY")
    print("=" * 70)
    print(f"  Total papers: {stats['total_papers']}")
    print(f"  Papers with TeX: {stats['papers_with_tex']}")
    print(f"  Papers with metadata: {stats['papers_with_metadata']}")
    print(f"  Papers with references: {stats['papers_with_references']}")
    print(f"  Total versions: {stats['total_versions']}")
    print(f"  Total .tex files: {stats['total_tex_files']}")
    print(f"  Total .bib files: {stats['total_bib_files']}")
    print("=" * 70)
    
    return stats

# Cháº¡y verification
stats = verify_data_structure()
```

---

## ğŸ“Š BÆ¯á»šC 8: XEM PERFORMANCE REPORT

### Cell 7: Load vÃ  hiá»ƒn thá»‹ metrics

```python
# Load metrics tá»« file
with open('performance_metrics.json', 'r') as f:
    metrics = json.load(f)

print("=" * 70)
print("ğŸ“ˆ FINAL PERFORMANCE REPORT")
print("   (Copy vÃ o Report.docx)")
print("=" * 70)

print("\nğŸ¯ TESTBED:")
print(f"   {metrics['testbed']}")

print("\nâ±ï¸  RUNNING TIME (Wall Time - End-to-End):")
print(f"   â€¢ Total: {metrics['total_wall_time_seconds']:.2f}s")
print(f"           ({metrics['total_wall_time_minutes']:.2f} min)")
print(f"           ({metrics['total_wall_time_hours']:.2f} hours)")
print(f"   â€¢ Papers processed: {metrics['papers_processed']}")
print(f"   â€¢ Avg per paper: {metrics['avg_time_per_paper']:.2f}s")

print("\nğŸ’¾ MEMORY FOOTPRINT:")
print(f"   â€¢ Maximum RAM: {metrics['max_ram_mb']:.2f} MB ({metrics['max_ram_gb']:.2f} GB)")
print(f"   â€¢ Disk increase: {metrics['disk_increase_mb']:.2f} MB ({metrics['disk_increase_gb']:.2f} GB)")
print(f"   â€¢ Output size: {metrics['output_size_mb']:.2f} MB ({metrics['output_size_gb']:.2f} GB)")

if stats:
    print("\nğŸ“Š DATA STATISTICS:")
    print(f"   â€¢ Total papers: {stats['total_papers']}")
    print(f"   â€¢ With TeX sources: {stats['papers_with_tex']}")
    print(f"   â€¢ With metadata: {stats['papers_with_metadata']}")
    print(f"   â€¢ With references: {stats['papers_with_references']}")
    print(f"   â€¢ Total versions: {stats['total_versions']}")
    print(f"   â€¢ Total .tex files: {stats['total_tex_files']}")
    print(f"   â€¢ Total .bib files: {stats['total_bib_files']}")

print("\n" + "=" * 70)
```

---

## ğŸ“¥ BÆ¯á»šC 9: DOWNLOAD Dá»® LIá»†U

### Option 1: Download trá»±c tiáº¿p (file < 100MB)

```python
import shutil
from google.colab import files

# NÃ©n dá»¯ liá»‡u
print("ğŸ“¦ Äang nÃ©n dá»¯ liá»‡u...")
shutil.make_archive('23127240_data', 'zip', '.', '23127240_data')

# Check size
size_mb = os.path.getsize('23127240_data.zip') / (1024**2)
print(f"âœ… ÄÃ£ táº¡o: 23127240_data.zip")
print(f"ğŸ“Š KÃ­ch thÆ°á»›c: {size_mb:.2f} MB")

if size_mb > 100:
    print("\nâš ï¸  File > 100MB, khuyáº¿n nghá»‹ upload lÃªn Google Drive")
    print("   ğŸ‘‰ Cháº¡y cell tiáº¿p theo")
else:
    print("\nâ¬‡ï¸  Downloading...")
    files.download('23127240_data.zip')
```

### Option 2: Upload to Google Drive (file lá»›n)

```python
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Copy to Drive
!cp 23127240_data.zip /content/drive/MyDrive/
!cp performance_metrics.json /content/drive/MyDrive/

print("âœ… ÄÃ£ upload lÃªn Google Drive:")
print("   - 23127240_data.zip")
print("   - performance_metrics.json")
```

---

## ğŸ¬ CHO DEMO VIDEO (â‰¤120 GIÃ‚Y)

Khi quay video demo, bao gá»“m:

### 1. Setup (15s)
- Má»Ÿ Colab, show Runtime = CPU-only
- Clone repo

### 2. Running (45s)
- Cháº¡y scraper
- Show logs: downloading, extracting, removing figures
- Show memory tracking

### 3. Results (45s)
- Show performance metrics
- Verify data structure
- Show 1 paper máº«u (tex/, metadata.json, references.json)

### 4. Voice explanation:
- "Scraper cháº¡y trÃªn Colab CPU-only theo yÃªu cáº§u Lab 1"
- "Tá»± Ä‘á»™ng Ä‘o wall time end-to-end vÃ  memory footprint"
- "Remove figures Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c"
- "Crawl references tá»« Semantic Scholar"

---

## âš ï¸ TROUBLESHOOTING

### Lá»—i "ModuleNotFoundError: No module named 'main'":

**NguyÃªn nhÃ¢n**: File `main.py` khÃ´ng tá»“n táº¡i hoáº·c cÃ³ tÃªn khÃ¡c trong thÆ° má»¥c `src/`

**Giáº£i phÃ¡p 1**: Kiá»ƒm tra tÃªn file thá»±c táº¿
```python
%cd /content/ScrapingData/23127240/src
!ls -la *.py
```

Sau Ä‘Ã³ thay `main.py` báº±ng tÃªn file Ä‘Ãºng.

**Giáº£i phÃ¡p 2**: Cháº¡y trá»±c tiáº¿p báº±ng terminal (KHUYáº¾N NGHá»Š)
```python
%cd /content/ScrapingData/23127240/src
# Thay 'main.py' báº±ng tÃªn file thá»±c táº¿
!python3 main.py
```

**Giáº£i phÃ¡p 3**: Náº¿u repository khÃ´ng cÃ³ file main
```python
# Clone láº¡i repository má»›i nháº¥t
%cd /content
!rm -rf ScrapingData
!git clone https://github.com/nhutphansayhi/ScrapingData.git
%cd ScrapingData/23127240
```

### Lá»—i Rate Limit (429):
```python
# Äá»£i 5 phÃºt
import time
time.sleep(300)
```

### Lá»—i Disk Full:
```python
# XÃ³a cache
!rm -rf ~/.cache/
!rm -rf /tmp/*
```

### Script cháº¡y quÃ¡ lÃ¢u:
- Giáº£m sá»‘ lÆ°á»£ng papers trong config.py
- Hoáº·c chia nhá» range

---

## ğŸ“ Há»– TRá»¢

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check logs trong cell scraper
2. Xem performance_metrics.json
3. LiÃªn há»‡: hlhdang@fit.hcmus.edu.vn

---

## âœ… CHECKLIST TRÆ¯á»šC KHI Ná»˜P

- [ ] Runtime = CPU-only
- [ ] Scraper cháº¡y thÃ nh cÃ´ng
- [ ] Performance metrics Ä‘Ã£ lÆ°u
- [ ] Data structure Ä‘Ãºng format (tex/, metadata.json, references.json)
- [ ] Figures Ä‘Ã£ Ä‘Æ°á»£c xÃ³a
- [ ] ÄÃ£ download/backup dá»¯ liá»‡u
- [ ] Report.docx Ä‘Ã£ ghi metrics
- [ ] Video demo â‰¤120s Ä‘Ã£ quay

---

**ChÃºc báº¡n thÃ nh cÃ´ng! ğŸš€**
