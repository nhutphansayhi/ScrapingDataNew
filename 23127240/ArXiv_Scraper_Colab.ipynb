{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e183a66",
   "metadata": {},
   "source": [
    "# ğŸš€ arXiv Scraper - Lab 1 Data Science\n",
    "## Student ID: 23127240\n",
    "\n",
    "**YÃªu cáº§u Lab 1:**\n",
    "- Testbed: Google Colab CPU-only mode\n",
    "- Äo wall time (end-to-end)\n",
    "- Äo memory footprint (max RAM, disk usage)\n",
    "- Thu tháº­p: TeX sources, metadata, references\n",
    "- Remove figures Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94af762f",
   "metadata": {},
   "source": [
    "## âš™ï¸ BÆ¯á»šC 1: Kiá»ƒm tra Runtime (CPU-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiá»ƒm tra runtime type (pháº£i lÃ  CPU theo yÃªu cáº§u Lab 1)\n",
    "import psutil\n",
    "import platform\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THÃ”NG TIN RUNTIME\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.2f} GB\")\n",
    "print(f\"Disk: {psutil.disk_usage('/').total / (1024**3):.2f} GB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Äáº£m báº£o khÃ´ng cÃ³ GPU (theo yÃªu cáº§u CPU-only)\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"âš ï¸ WARNING: GPU detected! Lab yÃªu cáº§u CPU-only mode\")\n",
    "        print(\"Chuyá»ƒn sang Runtime > Change runtime type > Hardware accelerator > None\")\n",
    "    else:\n",
    "        print(\"âœ… CPU-only mode - ÄÃºng yÃªu cáº§u Lab 1\")\n",
    "except:\n",
    "    print(\"âœ… CPU-only mode - ÄÃºng yÃªu cáº§u Lab 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ecc10",
   "metadata": {},
   "source": [
    "## ğŸ“¥ BÆ¯á»šC 2: Clone Source Code tá»« GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4869dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/nhutphansayhi/ScrapingDataNew.git\n",
    "%cd ScrapingDataNew/23127240\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b6305",
   "metadata": {},
   "source": [
    "## ğŸ“¦ BÆ¯á»šC 3: CÃ i Ä‘áº·t Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d0b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t\n",
    "!pip install -q arxiv requests beautifulsoup4 bibtexparser psutil\n",
    "\n",
    "# Verify installation\n",
    "import arxiv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bibtexparser\n",
    "import psutil\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"âœ… Táº¥t cáº£ thÆ° viá»‡n Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c04dc",
   "metadata": {},
   "source": [
    "## ğŸ“Š BÆ¯á»šC 4: Setup Performance Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69797c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class PerformanceMonitor:\n",
    "    \"\"\"Monitor Ä‘á»ƒ Ä‘o wall time vÃ  memory footprint theo yÃªu cáº§u Lab 1\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.initial_disk_mb = 0\n",
    "        self.max_ram_mb = 0\n",
    "        self.max_disk_mb = 0\n",
    "        self.paper_times = []\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Báº¯t Ä‘áº§u Ä‘o wall time (end-to-end)\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.initial_disk_mb = psutil.disk_usage('/').used / (1024**2)\n",
    "        initial_ram = psutil.virtual_memory().used / (1024**2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸš€ Báº¯t Ä‘áº§u scraping: {}\".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ğŸ“Š Initial disk: {:.2f} MB\".format(self.initial_disk_mb))\n",
    "        print(\"ğŸ’¾ Initial RAM: {:.2f} MB\".format(initial_ram))\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "    def update_metrics(self, paper_id=None, paper_time=None):\n",
    "        \"\"\"Cáº­p nháº­t metrics trong quÃ¡ trÃ¬nh scraping\"\"\"\n",
    "        ram_mb = psutil.virtual_memory().used / (1024**2)\n",
    "        self.max_ram_mb = max(self.max_ram_mb, ram_mb)\n",
    "        \n",
    "        disk_mb = psutil.disk_usage('/').used / (1024**2)\n",
    "        self.max_disk_mb = max(self.max_disk_mb, disk_mb)\n",
    "        \n",
    "        if paper_id and paper_time is not None:\n",
    "            self.paper_times.append({\n",
    "                'paper_id': paper_id,\n",
    "                'time_seconds': paper_time\n",
    "            })\n",
    "        \n",
    "    def finish(self, output_dir=None):\n",
    "        \"\"\"Káº¿t thÃºc vÃ  tÃ­nh toÃ¡n metrics\"\"\"\n",
    "        self.end_time = time.time()\n",
    "        total_time = self.end_time - self.start_time\n",
    "        disk_increase = self.max_disk_mb - self.initial_disk_mb\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ğŸ“Š PERFORMANCE METRICS (theo yÃªu cáº§u Lab 1)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Wall time\n",
    "        print(\"\\nâ±ï¸  WALL TIME (End-to-End):\")\n",
    "        print(\"   Total: {:.2f} seconds ({:.2f} minutes)\".format(total_time, total_time/60))\n",
    "        \n",
    "        if self.paper_times:\n",
    "            avg_time = sum(p['time_seconds'] for p in self.paper_times) / len(self.paper_times)\n",
    "            print(\"   Average per paper: {:.2f} seconds\".format(avg_time))\n",
    "            print(\"   Papers processed: {}\".format(len(self.paper_times)))\n",
    "        \n",
    "        # Memory footprint\n",
    "        print(\"\\nğŸ’¾ MEMORY FOOTPRINT:\")\n",
    "        print(\"   Maximum RAM used: {:.2f} MB ({:.2f} GB)\".format(self.max_ram_mb, self.max_ram_mb/1024))\n",
    "        current_ram = psutil.virtual_memory().used / (1024**2)\n",
    "        print(\"   Current RAM: {:.2f} MB\".format(current_ram))\n",
    "        \n",
    "        # Disk usage\n",
    "        print(\"\\nğŸ’¿ DISK USAGE:\")\n",
    "        print(\"   Maximum disk used: {:.2f} MB ({:.2f} GB)\".format(self.max_disk_mb, self.max_disk_mb/1024))\n",
    "        print(\"   Disk increase: {:.2f} MB ({:.2f} GB)\".format(disk_increase, disk_increase/1024))\n",
    "        \n",
    "        # Calculate output size\n",
    "        output_size_mb = 0\n",
    "        if output_dir and os.path.exists(output_dir):\n",
    "            total_size = sum(\n",
    "                os.path.getsize(os.path.join(dp, f))\n",
    "                for dp, dn, filenames in os.walk(output_dir)\n",
    "                for f in filenames\n",
    "            )\n",
    "            output_size_mb = total_size / (1024**2)\n",
    "            print(\"   Output data size: {:.2f} MB ({:.2f} GB)\".format(output_size_mb, output_size_mb/1024))\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return {\n",
    "            'testbed': 'Google Colab CPU-only',\n",
    "            'total_wall_time_seconds': total_time,\n",
    "            'total_wall_time_minutes': total_time / 60,\n",
    "            'total_wall_time_hours': total_time / 3600,\n",
    "            'max_ram_mb': self.max_ram_mb,\n",
    "            'max_ram_gb': self.max_ram_mb / 1024,\n",
    "            'disk_increase_mb': disk_increase,\n",
    "            'disk_increase_gb': disk_increase / 1024,\n",
    "            'output_size_mb': output_size_mb,\n",
    "            'output_size_gb': output_size_mb / 1024,\n",
    "            'papers_processed': len(self.paper_times),\n",
    "            'avg_time_per_paper': sum(p['time_seconds'] for p in self.paper_times) / len(self.paper_times) if self.paper_times else 0,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Khá»Ÿi táº¡o monitor\n",
    "monitor = PerformanceMonitor()\n",
    "print(\"âœ… Performance Monitor Ä‘Ã£ sáºµn sÃ ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6ff18",
   "metadata": {},
   "source": [
    "## ğŸ”§ BÆ¯á»šC 5: Cháº¡y Scraper vá»›i Performance Monitoring\n",
    "\n",
    "**LÆ°u Ã½:** Script sáº½ tá»± Ä‘á»™ng:\n",
    "- Scrape metadata tá»« arXiv API\n",
    "- Download TeX sources (.tar.gz)\n",
    "- Remove figures (png, jpg, pdf, eps)\n",
    "- Crawl references tá»« Semantic Scholar\n",
    "- LÆ°u theo cáº¥u trÃºc yÃªu cáº§u Lab 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cb551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Báº®T Äáº¦U ÄO WALL TIME\n",
    "monitor.start()\n",
    "\n",
    "try:\n",
    "    print(\"ğŸ”„ Äang cháº¡y scraper...\")\n",
    "    print(\"\\nQuy trÃ¬nh (theo Ä‘á» bÃ i Lab 1):\")\n",
    "    print(\"  1ï¸âƒ£  Entry Discovery: arXiv API\")\n",
    "    print(\"  2ï¸âƒ£  Source Download: .tar.gz extraction\")\n",
    "    print(\"  3ï¸âƒ£  Figure Removal: XÃ³a png, jpg, pdf, eps\")\n",
    "    print(\"  4ï¸âƒ£  Reference Crawling: Semantic Scholar API\")\n",
    "    print(\"  5ï¸âƒ£  Data Organization: tex/, metadata.json, references.json\")\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    \n",
    "    # Di chuyá»ƒn vÃ o thÆ° má»¥c src\n",
    "    os.chdir('/content/ScrapingDataNew/23127240/src')\n",
    "    \n",
    "    # Cháº¡y scraper báº±ng subprocess (cháº¡y nhÆ° terminal command)\n",
    "    result = subprocess.run(\n",
    "        ['python3', 'main.py'],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=7200  # 2 hours timeout\n",
    "    )\n",
    "    \n",
    "    # Hiá»ƒn thá»‹ output\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(result.stderr)\n",
    "    \n",
    "    # Check exit code\n",
    "    if result.returncode != 0:\n",
    "        print(f\"\\nâš ï¸  Scraper thoÃ¡t vá»›i code: {result.returncode}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… Scraper hoÃ n táº¥t thÃ nh cÃ´ng!\")\n",
    "    \n",
    "    # Update metrics\n",
    "    monitor.update_metrics()\n",
    "    \n",
    "    # Vá» thÆ° má»¥c gá»‘c\n",
    "    os.chdir('/content/ScrapingDataNew/23127240')\n",
    "    \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"\\nâš ï¸  Timeout: Scraping vÆ°á»£t quÃ¡ 2 giá»\")\n",
    "    os.chdir('/content/ScrapingDataNew/23127240')\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâš ï¸  Scraping bá»‹ ngáº¯t bá»Ÿi user\")\n",
    "    os.chdir('/content/ScrapingDataNew/23127240')\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Lá»—i: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    os.chdir('/content/ScrapingDataNew/23127240')\n",
    "finally:\n",
    "    # Káº¾T THÃšC ÄO WALL TIME\n",
    "    metrics = monitor.finish(output_dir=\"23127240_data\")\n",
    "    \n",
    "    # LÆ°u metrics\n",
    "    with open('performance_metrics.json', 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    print(\"\\nğŸ’¾ Metrics Ä‘Ã£ lÆ°u vÃ o: performance_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e864822",
   "metadata": {},
   "source": [
    "### ğŸ”„ CÃ¡ch 2: Cháº¡y trá»±c tiáº¿p báº±ng terminal (ÄÆ N GIáº¢N NHáº¤T)\n",
    "\n",
    "Náº¿u cÃ¡ch trÃªn lá»—i, dÃ¹ng cÃ¡ch nÃ y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Báº®T Äáº¦U ÄO WALL TIME\n",
    "monitor.start()\n",
    "\n",
    "# Di chuyá»ƒn vÃ o thÆ° má»¥c src\n",
    "%cd /content/ScrapingDataNew/23127240/src\n",
    "\n",
    "# Cháº¡y scraper\n",
    "!python3 main.py\n",
    "\n",
    "# Vá» thÆ° má»¥c gá»‘c\n",
    "%cd /content/ScrapingDataNew/23127240\n",
    "\n",
    "# Káº¾T THÃšC ÄO WALL TIME\n",
    "metrics = monitor.finish(output_dir=\"23127240_data\")\n",
    "\n",
    "# LÆ°u metrics\n",
    "import json\n",
    "with open('performance_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nğŸ’¾ Metrics Ä‘Ã£ lÆ°u vÃ o: performance_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47cd802",
   "metadata": {},
   "source": [
    "## ğŸ“ BÆ¯á»šC 6: Kiá»ƒm tra Cáº¥u trÃºc Dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def verify_data_structure(data_dir=\"23127240_data\"):\n",
    "    \"\"\"Kiá»ƒm tra cáº¥u trÃºc dá»¯ liá»‡u theo yÃªu cáº§u Lab 1\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ“ KIá»‚M TRA Cáº¤U TRÃšC Dá»® LIá»†U\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"âŒ ThÆ° má»¥c {data_dir} khÃ´ng tá»“n táº¡i!\")\n",
    "        return\n",
    "    \n",
    "    papers = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    print(f\"\\nğŸ“Š Tá»•ng sá»‘ papers: {len(papers)}\")\n",
    "    \n",
    "    stats = {\n",
    "        'total_papers': len(papers),\n",
    "        'papers_with_tex': 0,\n",
    "        'papers_with_metadata': 0,\n",
    "        'papers_with_references': 0,\n",
    "        'total_versions': 0,\n",
    "        'total_tex_files': 0,\n",
    "        'total_bib_files': 0,\n",
    "        'total_references': 0\n",
    "    }\n",
    "    \n",
    "    # Check first 10 papers in detail\n",
    "    for paper_id in sorted(papers)[:10]:\n",
    "        paper_path = os.path.join(data_dir, paper_id)\n",
    "        print(f\"\\nğŸ“„ {paper_id}:\")\n",
    "        \n",
    "        # Check tex folder\n",
    "        tex_path = os.path.join(paper_path, \"tex\")\n",
    "        if os.path.exists(tex_path):\n",
    "            versions = [d for d in os.listdir(tex_path) if os.path.isdir(os.path.join(tex_path, d))]\n",
    "            stats['papers_with_tex'] += 1\n",
    "            stats['total_versions'] += len(versions)\n",
    "            print(f\"   âœ… tex/ - {len(versions)} version(s)\")\n",
    "            \n",
    "            # Count .tex and .bib files\n",
    "            for version in versions:\n",
    "                version_path = os.path.join(tex_path, version)\n",
    "                for root, dirs, files in os.walk(version_path):\n",
    "                    stats['total_tex_files'] += len([f for f in files if f.endswith('.tex')])\n",
    "                    stats['total_bib_files'] += len([f for f in files if f.endswith('.bib')])\n",
    "        else:\n",
    "            print(f\"   âŒ tex/ missing\")\n",
    "        \n",
    "        # Check metadata.json\n",
    "        metadata_path = os.path.join(paper_path, \"metadata.json\")\n",
    "        if os.path.exists(metadata_path):\n",
    "            stats['papers_with_metadata'] += 1\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                metadata = json.load(f)\n",
    "                title = metadata.get('title', 'N/A')\n",
    "                print(f\"   âœ… metadata.json - {title[:60]}...\")\n",
    "        else:\n",
    "            print(f\"   âŒ metadata.json missing\")\n",
    "        \n",
    "        # Check references.json\n",
    "        ref_path = os.path.join(paper_path, \"references.json\")\n",
    "        if os.path.exists(ref_path):\n",
    "            stats['papers_with_references'] += 1\n",
    "            with open(ref_path, 'r') as f:\n",
    "                refs = json.load(f)\n",
    "                stats['total_references'] += len(refs)\n",
    "                print(f\"   âœ… references.json - {len(refs)} reference(s)\")\n",
    "        else:\n",
    "            print(f\"   âŒ references.json missing\")\n",
    "    \n",
    "    # Count all papers\n",
    "    for paper_id in papers:\n",
    "        paper_path = os.path.join(data_dir, paper_id)\n",
    "        if os.path.exists(os.path.join(paper_path, \"tex\")):\n",
    "            stats['papers_with_tex'] += 1\n",
    "        if os.path.exists(os.path.join(paper_path, \"metadata.json\")):\n",
    "            stats['papers_with_metadata'] += 1\n",
    "        if os.path.exists(os.path.join(paper_path, \"references.json\")):\n",
    "            stats['papers_with_references'] += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“Š STATISTICS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    for key, value in stats.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    # Calculate success rates\n",
    "    if stats['total_papers'] > 0:\n",
    "        print(\"\\nğŸ“ˆ SUCCESS RATES:\")\n",
    "        print(f\"   TeX success: {stats['papers_with_tex']/stats['total_papers']*100:.1f}%\")\n",
    "        print(f\"   Metadata success: {stats['papers_with_metadata']/stats['total_papers']*100:.1f}%\")\n",
    "        print(f\"   References success: {stats['papers_with_references']/stats['total_papers']*100:.1f}%\")\n",
    "        if stats['papers_with_references'] > 0:\n",
    "            avg_refs = stats['total_references'] / stats['papers_with_references']\n",
    "            print(f\"   Avg references per paper: {avg_refs:.1f}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Cháº¡y verification\n",
    "stats = verify_data_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074abf1c",
   "metadata": {},
   "source": [
    "## ğŸ“Š BÆ¯á»šC 7: Performance Report cho Report.docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load performance metrics\n",
    "with open('performance_metrics.json', 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“ˆ FINAL PERFORMANCE REPORT (copy vÃ o Report.docx)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nğŸ¯ TESTBED: Google Colab CPU-only mode\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nâ±ï¸  RUNNING TIME:\")\n",
    "print(f\"   â€¢ Total wall time: {metrics['total_wall_time_seconds']:.2f}s ({metrics['total_wall_time_seconds']/60:.2f} min)\")\n",
    "print(f\"   â€¢ Average time per paper: {metrics['avg_time_per_paper']:.2f}s\")\n",
    "print(f\"   â€¢ Papers processed: {metrics['papers_processed']}\")\n",
    "\n",
    "print(\"\\nğŸ’¾ MEMORY FOOTPRINT:\")\n",
    "print(f\"   â€¢ Maximum RAM used: {metrics['max_ram_mb']:.2f} MB ({metrics['max_ram_mb']/1024:.2f} GB)\")\n",
    "print(f\"   â€¢ Maximum disk used: {metrics['max_disk_mb']:.2f} MB ({metrics['max_disk_mb']/1024:.2f} GB)\")\n",
    "if 'disk_increase_mb' in metrics:\n",
    "    print(f\"   â€¢ Disk increase: {metrics['disk_increase_mb']:.2f} MB ({metrics['disk_increase_mb']/1024:.2f} GB)\")\n",
    "\n",
    "print(\"\\nğŸ“Š DATA STATISTICS:\")\n",
    "if stats:\n",
    "    print(f\"   â€¢ Total papers scraped: {stats['total_papers']}\")\n",
    "    print(f\"   â€¢ Papers with TeX: {stats['papers_with_tex']}\")\n",
    "    print(f\"   â€¢ Papers with metadata: {stats['papers_with_metadata']}\")\n",
    "    print(f\"   â€¢ Papers with references: {stats['papers_with_references']}\")\n",
    "    print(f\"   â€¢ Total versions: {stats['total_versions']}\")\n",
    "    print(f\"   â€¢ Total .tex files: {stats['total_tex_files']}\")\n",
    "    print(f\"   â€¢ Total .bib files: {stats['total_bib_files']}\")\n",
    "    print(f\"   â€¢ Total references: {stats['total_references']}\")\n",
    "    if stats['papers_with_references'] > 0:\n",
    "        avg_refs = stats['total_references'] / stats['papers_with_references']\n",
    "        print(f\"   â€¢ Average references per paper: {avg_refs:.1f}\")\n",
    "    \n",
    "    # Success rates\n",
    "    if stats['total_papers'] > 0:\n",
    "        print(f\"\\nğŸ“ˆ SUCCESS RATES:\")\n",
    "        print(f\"   â€¢ Overall success rate: {stats['papers_with_metadata']/stats['total_papers']*100:.1f}%\")\n",
    "        print(f\"   â€¢ TeX extraction rate: {stats['papers_with_tex']/stats['total_papers']*100:.1f}%\")\n",
    "        print(f\"   â€¢ Reference crawling rate: {stats['papers_with_references']/stats['total_papers']*100:.1f}%\")\n",
    "\n",
    "print(\"\\nâœ… Copy metrics nÃ y vÃ o Report.docx!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedf0e3e",
   "metadata": {},
   "source": [
    "## ğŸ“¥ BÆ¯á»šC 8: Download Dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "# NÃ©n dá»¯ liá»‡u\n",
    "print(\"ğŸ“¦ Äang nÃ©n dá»¯ liá»‡u...\")\n",
    "shutil.make_archive('23127240_data', 'zip', '.', '23127240_data')\n",
    "print(f\"âœ… ÄÃ£ táº¡o 23127240_data.zip\")\n",
    "\n",
    "# Kiá»ƒm tra kÃ­ch thÆ°á»›c\n",
    "size_mb = os.path.getsize('23127240_data.zip') / (1024**2)\n",
    "print(f\"ğŸ“Š KÃ­ch thÆ°á»›c: {size_mb:.2f} MB\")\n",
    "\n",
    "if size_mb > 100:\n",
    "    print(\"âš ï¸ File lá»›n hÆ¡n 100MB, khuyáº¿n nghá»‹ upload lÃªn Google Drive\")\n",
    "    print(\"Cháº¡y cell tiáº¿p theo Ä‘á»ƒ upload lÃªn Drive\")\n",
    "else:\n",
    "    print(\"\\nâ¬‡ï¸ Báº¯t Ä‘áº§u download...\")\n",
    "    files.download('23127240_data.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6ed4a1",
   "metadata": {},
   "source": [
    "## â˜ï¸ BÆ¯á»šC 9: Upload lÃªn Google Drive (náº¿u file quÃ¡ lá»›n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0315a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy vÃ o Drive\n",
    "!cp 23127240_data.zip /content/drive/MyDrive/\n",
    "!cp performance_metrics.json /content/drive/MyDrive/\n",
    "\n",
    "print(\"âœ… ÄÃ£ upload vÃ o Google Drive:\")\n",
    "print(\"   - 23127240_data.zip\")\n",
    "print(\"   - performance_metrics.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3903d6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ GHI CHÃš\n",
    "\n",
    "### YÃªu cáº§u Lab 1 Ä‘Ã£ hoÃ n thÃ nh:\n",
    "- âœ… Testbed: Google Colab CPU-only mode\n",
    "- âœ… Wall time measurement (end-to-end)\n",
    "- âœ… Memory footprint (max RAM, disk usage)\n",
    "- âœ… Scrape: TeX sources, metadata, references\n",
    "- âœ… Remove figures Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c\n",
    "- âœ… Cáº¥u trÃºc theo format yÃªu cáº§u\n",
    "\n",
    "### Rate Limiting:\n",
    "- Semantic Scholar: 1 req/s, 100 req/5min\n",
    "- Script cÃ³ built-in retry mechanism\n",
    "\n",
    "### Demo Video (â‰¤120s):\n",
    "1. Setup (15s): Má»Ÿ Colab, check CPU-only, clone repo\n",
    "2. Running (45s): Cháº¡y scraper, show logs\n",
    "3. Results (45s): Performance metrics, verify structure\n",
    "4. Voice: Giáº£i thÃ­ch scraper design vÃ  reasoning\n",
    "\n",
    "### LiÃªn há»‡:\n",
    "- Instructor: hlhdang@fit.hcmus.edu.vn\n",
    "\n",
    "---\n",
    "\n",
    "**ChÃºc báº¡n scraping thÃ nh cÃ´ng! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
