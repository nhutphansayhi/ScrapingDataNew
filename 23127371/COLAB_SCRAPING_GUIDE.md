# HÆ¯á»šNG DáºªN CHáº Y SCRAPER TRÃŠN GOOGLE COLAB

## ğŸ“‹ Tá»”NG QUAN

HÆ°á»›ng dáº«n nÃ y giÃºp báº¡n cháº¡y arXiv scraper trÃªn Google Colab Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u cÃ¡c bÃ i bÃ¡o cÃ²n láº¡i theo yÃªu cáº§u Lab 1.

**YÃªu cáº§u tá»« Lab 1:**
- Testbed: Google Colab instance, CPU-only mode
- Äo wall time (thá»i gian cháº¡y end-to-end)
- Äo memory footprint (RAM tá»‘i Ä‘a, disk usage)
- Thu tháº­p: TeX sources, metadata.json, references.json
- Loáº¡i bá» táº¥t cáº£ hÃ¬nh áº£nh (figures) Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c

---

## ğŸš€ BÆ¯á»šC 1: Má» GOOGLE COLAB

1. Truy cáº­p: https://colab.research.google.com/
2. ÄÄƒng nháº­p Google Account
3. Táº¡o notebook má»›i: **File > New notebook**
4. Äá»•i tÃªn notebook: `ArXiv_Scraper_23127240.ipynb`

---

## âš™ï¸ BÆ¯á»šC 2: SETUP MÃ”I TRÆ¯á»œNG

### Cell 1: Kiá»ƒm tra Runtime (CPU-only)

```python
# Kiá»ƒm tra runtime type (pháº£i lÃ  CPU theo yÃªu cáº§u Lab 1)
import psutil
import platform

print("=" * 60)
print("THÃ”NG TIN RUNTIME")
print("=" * 60)
print(f"OS: {platform.system()} {platform.release()}")
print(f"CPU cores: {psutil.cpu_count()}")
print(f"RAM: {psutil.virtual_memory().total / (1024**3):.2f} GB")
print(f"Disk: {psutil.disk_usage('/').total / (1024**3):.2f} GB")
print("=" * 60)

# Äáº£m báº£o khÃ´ng cÃ³ GPU (theo yÃªu cáº§u CPU-only)
try:
    import torch
    if torch.cuda.is_available():
        print("âš ï¸ WARNING: GPU detected! Lab yÃªu cáº§u CPU-only mode")
        print("Chuyá»ƒn sang Runtime > Change runtime type > Hardware accelerator > None")
    else:
        print("âœ… CPU-only mode - ÄÃºng yÃªu cáº§u Lab 1")
except:
    print("âœ… CPU-only mode - ÄÃºng yÃªu cáº§u Lab 1")
```

**Action**: Cháº¡y cell nÃ y, náº¿u tháº¥y GPU warning thÃ¬ Ä‘á»•i sang CPU:
- Runtime > Change runtime type > Hardware accelerator > **None**

---

### Cell 2: Clone Repository

```python
# Clone source code tá»« GitHub
!git clone https://github.com/nhutphansayhi/ScrapingData.git
%cd ScrapingData/23127371
!ls -la
```

---

### Cell 3: CÃ i Ä‘áº·t Dependencies

```python
# CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t
!pip install -q arxiv requests beautifulsoup4 bibtexparser psutil

# Verify installation
import arxiv
import requests
from bs4 import BeautifulSoup
import bibtexparser
import psutil
import json
import time

print("âœ… Táº¥t cáº£ thÆ° viá»‡n Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t!")
```

---

## ğŸ“Š BÆ¯á»šC 3: Táº O MONITORING SCRIPT

### Cell 4: Setup Memory & Time Tracking

```python
import psutil
import time
import os
from datetime import datetime

class PerformanceMonitor:
    """Theo dÃµi performance theo yÃªu cáº§u Lab 1"""
    
    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.max_ram_mb = 0
        self.max_disk_mb = 0
        self.paper_times = []
        
    def start(self):
        """Báº¯t Ä‘áº§u Ä‘o wall time"""
        self.start_time = time.time()
        print(f"ğŸš€ Báº¯t Ä‘áº§u scraping: {datetime.now()}")
        
    def update(self, paper_id, paper_time):
        """Cáº­p nháº­t metrics cho má»—i paper"""
        # RAM usage
        ram_mb = psutil.virtual_memory().used / (1024**2)
        self.max_ram_mb = max(self.max_ram_mb, ram_mb)
        
        # Disk usage
        disk_mb = psutil.disk_usage('/').used / (1024**2)
        self.max_disk_mb = max(self.max_disk_mb, disk_mb)
        
        # Paper processing time
        self.paper_times.append({
            'paper_id': paper_id,
            'time_seconds': paper_time
        })
        
    def finish(self):
        """Káº¿t thÃºc vÃ  tÃ­nh toÃ¡n metrics"""
        self.end_time = time.time()
        total_time = self.end_time - self.start_time
        
        print("\n" + "=" * 70)
        print("ğŸ“Š PERFORMANCE METRICS (theo yÃªu cáº§u Lab 1)")
        print("=" * 70)
        
        # Wall time
        print(f"\nâ±ï¸  WALL TIME (End-to-End):")
        print(f"   Total: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
        
        if self.paper_times:
            avg_time = sum(p['time_seconds'] for p in self.paper_times) / len(self.paper_times)
            print(f"   Average per paper: {avg_time:.2f} seconds")
            print(f"   Papers processed: {len(self.paper_times)}")
        
        # Memory footprint
        print(f"\nğŸ’¾ MEMORY FOOTPRINT:")
        print(f"   Maximum RAM used: {self.max_ram_mb:.2f} MB ({self.max_ram_mb/1024:.2f} GB)")
        current_ram = psutil.virtual_memory().used / (1024**2)
        print(f"   Current RAM: {current_ram:.2f} MB")
        
        # Disk usage
        print(f"\nğŸ’¿ DISK USAGE:")
        print(f"   Maximum disk used: {self.max_disk_mb:.2f} MB ({self.max_disk_mb/1024:.2f} GB)")
        
        # Calculate output size
        output_dir = "23127240_data"
        if os.path.exists(output_dir):
            total_size = sum(
                os.path.getsize(os.path.join(dp, f))
                for dp, dn, filenames in os.walk(output_dir)
                for f in filenames
            )
            print(f"   Output data size: {total_size/(1024**2):.2f} MB ({total_size/(1024**3):.2f} GB)")
        
        print("=" * 70)
        
        return {
            'total_wall_time_seconds': total_time,
            'max_ram_mb': self.max_ram_mb,
            'max_disk_mb': self.max_disk_mb,
            'papers_processed': len(self.paper_times),
            'avg_time_per_paper': avg_time if self.paper_times else 0,
            'paper_times': self.paper_times
        }

# Khá»Ÿi táº¡o monitor
monitor = PerformanceMonitor()
print("âœ… Performance Monitor Ä‘Ã£ sáºµn sÃ ng!")
```

---

## ğŸ”§ BÆ¯á»šC 4: CHáº Y SCRAPER

### Cell 5: Cháº¡y Main Script vá»›i Monitoring

```python
import sys
import os

# ThÃªm src vÃ o Python path
sys.path.insert(0, '/content/ScrapingData/23127371/src')

# Import scraper modules
from main import main as run_scraper
from config import *

# Báº®T Äáº¦U ÄO WALL TIME
monitor.start()

try:
    # Cháº¡y scraper (end-to-end)
    # Script sáº½ tá»± Ä‘á»™ng:
    # - Scrape metadata
    # - Download TeX sources
    # - Remove figures (theo yÃªu cáº§u Lab 1)
    # - Crawl references tá»« Semantic Scholar
    # - LÆ°u vÃ o 23127240_data/
    
    print("ğŸ”„ Äang cháº¡y scraper...")
    print("   - Entry discovery: arXiv API")
    print("   - Source download: .tar.gz extraction")
    print("   - Figure removal: Tá»± Ä‘á»™ng xÃ³a áº£nh")
    print("   - Reference crawling: Semantic Scholar API")
    print("\n")
    
    run_scraper()
    
except Exception as e:
    print(f"âŒ Lá»—i: {e}")
    import traceback
    traceback.print_exc()
finally:
    # Káº¾T THÃšC ÄO WALL TIME
    metrics = monitor.finish()
    
    # LÆ°u metrics Ä‘á»ƒ bÃ¡o cÃ¡o
    with open('performance_metrics.json', 'w') as f:
        json.dump(metrics, f, indent=2)
    
    print("\nâœ… Metrics Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o performance_metrics.json")
```

**LÆ°u Ã½**: Script sáº½ cháº¡y trong vÃ i phÃºt Ä‘áº¿n vÃ i giá» tÃ¹y sá»‘ lÆ°á»£ng papers. Monitor sáº½ tá»± Ä‘á»™ng Ä‘o wall time vÃ  memory.

---

## ğŸ“ BÆ¯á»šC 5: KIá»‚M TRA OUTPUT

### Cell 6: Verify Data Structure

```python
import os
import json

def verify_data_structure(data_dir="23127240_data"):
    """Kiá»ƒm tra cáº¥u trÃºc dá»¯ liá»‡u theo yÃªu cáº§u Lab 1"""
    
    print("=" * 70)
    print("ğŸ“ KIá»‚M TRA Cáº¤U TRÃšC Dá»® LIá»†U")
    print("=" * 70)
    
    if not os.path.exists(data_dir):
        print(f"âŒ ThÆ° má»¥c {data_dir} khÃ´ng tá»“n táº¡i!")
        return
    
    papers = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]
    print(f"\nğŸ“Š Tá»•ng sá»‘ papers: {len(papers)}")
    
    stats = {
        'total_papers': len(papers),
        'papers_with_tex': 0,
        'papers_with_metadata': 0,
        'papers_with_references': 0,
        'total_versions': 0,
        'total_tex_files': 0,
        'total_bib_files': 0,
        'papers_missing_figures': 0
    }
    
    for paper_id in papers[:5]:  # Check first 5 papers
        paper_path = os.path.join(data_dir, paper_id)
        print(f"\nğŸ“„ {paper_id}:")
        
        # Check tex folder
        tex_path = os.path.join(paper_path, "tex")
        if os.path.exists(tex_path):
            versions = os.listdir(tex_path)
            stats['papers_with_tex'] += 1
            stats['total_versions'] += len(versions)
            print(f"   âœ… tex/ - {len(versions)} version(s)")
            
            # Count .tex and .bib files
            for version in versions:
                version_path = os.path.join(tex_path, version)
                for root, dirs, files in os.walk(version_path):
                    stats['total_tex_files'] += len([f for f in files if f.endswith('.tex')])
                    stats['total_bib_files'] += len([f for f in files if f.endswith('.bib')])
        else:
            print(f"   âŒ tex/ missing")
        
        # Check metadata.json
        metadata_path = os.path.join(paper_path, "metadata.json")
        if os.path.exists(metadata_path):
            stats['papers_with_metadata'] += 1
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
                print(f"   âœ… metadata.json - Title: {metadata.get('title', 'N/A')[:50]}...")
        else:
            print(f"   âŒ metadata.json missing")
        
        # Check references.json
        ref_path = os.path.join(paper_path, "references.json")
        if os.path.exists(ref_path):
            stats['papers_with_references'] += 1
            with open(ref_path, 'r') as f:
                refs = json.load(f)
                print(f"   âœ… references.json - {len(refs)} reference(s) with arXiv IDs")
        else:
            print(f"   âŒ references.json missing")
    
    print("\n" + "=" * 70)
    print("ğŸ“Š STATISTICS SUMMARY")
    print("=" * 70)
    for key, value in stats.items():
        print(f"   {key}: {value}")
    print("=" * 70)
    
    return stats

# Cháº¡y verification
stats = verify_data_structure()
```

---

## ğŸ“¥ BÆ¯á»šC 6: DOWNLOAD Dá»® LIá»†U

### Cell 7: NÃ©n vÃ  Download

```python
import shutil
from google.colab import files

# NÃ©n dá»¯ liá»‡u
print("ğŸ“¦ Äang nÃ©n dá»¯ liá»‡u...")
shutil.make_archive('23127240_data', 'zip', '.', '23127240_data')
print(f"âœ… ÄÃ£ táº¡o 23127240_data.zip")

# Kiá»ƒm tra kÃ­ch thÆ°á»›c
size_mb = os.path.getsize('23127240_data.zip') / (1024**2)
print(f"ğŸ“Š KÃ­ch thÆ°á»›c: {size_mb:.2f} MB")

if size_mb > 100:
    print("âš ï¸ File lá»›n hÆ¡n 100MB, cÃ³ thá»ƒ download cháº­m")
    print("ğŸ’¡ Khuyáº¿n nghá»‹: Upload lÃªn Google Drive thay vÃ¬ download trá»±c tiáº¿p")
else:
    print("\nâ¬‡ï¸ Báº¯t Ä‘áº§u download...")
    files.download('23127240_data.zip')
```

**Náº¿u file quÃ¡ lá»›n, upload lÃªn Google Drive:**

```python
# Cell 8: Upload to Google Drive (náº¿u file quÃ¡ lá»›n)
from google.colab import drive
drive.mount('/content/drive')

# Copy vÃ o Drive
!cp 23127240_data.zip /content/drive/MyDrive/
print("âœ… ÄÃ£ upload vÃ o Google Drive > MyDrive > 23127240_data.zip")
```

---

## ğŸ“Š BÆ¯á»šC 7: XEM PERFORMANCE REPORT

### Cell 9: Load vÃ  Hiá»ƒn thá»‹ Metrics

```python
# Load performance metrics
with open('performance_metrics.json', 'r') as f:
    metrics = json.load(f)

print("=" * 70)
print("ğŸ“ˆ FINAL PERFORMANCE REPORT (cho Report.docx)")
print("=" * 70)

print("\nğŸ¯ YÃŠU Cáº¦U LAB 1 - TESTBED: Google Colab CPU-only")
print("-" * 70)

print("\nâ±ï¸  RUNNING TIME:")
print(f"   â€¢ Total wall time: {metrics['total_wall_time_seconds']:.2f}s ({metrics['total_wall_time_seconds']/60:.2f} min)")
print(f"   â€¢ Average time per paper: {metrics['avg_time_per_paper']:.2f}s")
print(f"   â€¢ Papers processed: {metrics['papers_processed']}")

print("\nğŸ’¾ MEMORY FOOTPRINT:")
print(f"   â€¢ Maximum RAM used: {metrics['max_ram_mb']:.2f} MB ({metrics['max_ram_mb']/1024:.2f} GB)")
print(f"   â€¢ Maximum disk used: {metrics['max_disk_mb']:.2f} MB ({metrics['max_disk_mb']/1024:.2f} GB)")

print("\nğŸ“Š DATA STATISTICS:")
if stats:
    print(f"   â€¢ Total papers: {stats['total_papers']}")
    print(f"   â€¢ Papers with TeX: {stats['papers_with_tex']}")
    print(f"   â€¢ Total versions: {stats['total_versions']}")
    print(f"   â€¢ Total .tex files: {stats['total_tex_files']}")
    print(f"   â€¢ Total .bib files: {stats['total_bib_files']}")

print("\nâœ… Copy metrics nÃ y vÃ o Report.docx!")
print("=" * 70)
```

---

## ğŸ¬ BÆ¯á»šC 8: GHI CHÃš CHO DEMO VIDEO

Khi quay video demo (â‰¤120s), hÃ£y bao gá»“m:

1. **Setup (15s)**:
   - Má»Ÿ Colab
   - Show Runtime = CPU-only
   - Clone repo

2. **Running (45s)**:
   - Cháº¡y scraper
   - Show logs: downloading, extracting, removing figures
   - Show memory tracking

3. **Results (45s)**:
   - Show performance metrics
   - Verify data structure
   - Show má»™t paper máº«u (tex/, metadata.json, references.json)

4. **Voice explanation**:
   - "Scraper cháº¡y trÃªn Colab CPU-only theo yÃªu cáº§u Lab 1"
   - "Tá»± Ä‘á»™ng Ä‘o wall time vÃ  memory footprint"
   - "Remove figures Ä‘á»ƒ giáº£m kÃ­ch thÆ°á»›c"
   - "Crawl references tá»« Semantic Scholar"

---

## âš ï¸ LÆ¯U Ã QUAN TRá»ŒNG

### Rate Limiting (Semantic Scholar)
- **1 request/second**, 100 requests/5 minutes
- Script cÃ³ built-in retry mechanism
- Náº¿u gáº·p 429 error, Ä‘á»£i 5 phÃºt rá»“i cháº¡y láº¡i

### Colab Timeout
- Free tier: 12h max runtime
- Náº¿u scraping quÃ¡ nhiá»u papers, chia nhá» range

### Disk Space
- Colab free: ~100GB disk
- Monitor disk usage trong Cell 6
- Náº¿u full, download tá»«ng batch

### Figures Removal
- Script tá»± Ä‘á»™ng xÃ³a: .png, .jpg, .jpeg, .pdf, .eps
- Giáº£m ~70-80% kÃ­ch thÆ°á»›c theo kinh nghiá»‡m

---

## ğŸ†˜ TROUBLESHOOTING

**Lá»—i "No module named 'arxiv'":**
```python
!pip install --upgrade arxiv
```

**Lá»—i "Rate limit exceeded":**
```python
# Äá»£i 5 phÃºt rá»“i cháº¡y láº¡i
import time
time.sleep(300)
```

**Lá»—i "Disk full":**
```python
# XÃ³a cache
!rm -rf ~/.cache/
!rm -rf /tmp/*
```

**Script cháº¡y quÃ¡ lÃ¢u:**
- Giáº£m MAX_PAPERS trong config.py
- Hoáº·c chia nhá» paper range

---

## ğŸ“ Há»– TRá»¢

Náº¿u gáº·p váº¥n Ä‘á»:
1. Kiá»ƒm tra logs trong Cell 5
2. Xem performance_metrics.json
3. LiÃªn há»‡: hlhdang@fit.hcmus.edu.vn

---

**ChÃºc báº¡n scraping thÃ nh cÃ´ng! ğŸš€**
